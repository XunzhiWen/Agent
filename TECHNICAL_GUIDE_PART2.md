# LangGraph Gemini Agent - è¯¦ç»†æŠ€æœ¯æŒ‡å— (ç»­)

> è¿™æ˜¯æŠ€æœ¯æŒ‡å—çš„ç¬¬äºŒéƒ¨åˆ†,åŒ…å«è°ƒè¯•è„šæœ¬ã€å·¥ä½œæµç¨‹è¯¦è§£ã€æœ€ä½³å®è·µç­‰å†…å®¹

## è°ƒè¯•è„šæœ¬

### ğŸ“„ `debug_models.py` (21è¡Œ)

ç”¨äºæµ‹è¯• API è¿æ¥å¹¶åˆ—å‡ºå¯ç”¨çš„ Gemini æ¨¡å‹ã€‚

#### å®Œæ•´ä»£ç é€è¡Œè§£æ

```python
# ç¬¬1-3è¡Œ: å¯¼å…¥
import os
import google.generativeai as genai
from dotenv import load_dotenv
```

**ç¬¬1è¡Œ**: `import os`
- ç”¨äºè®¿é—®ç¯å¢ƒå˜é‡

**ç¬¬2è¡Œ**: `import google.generativeai as genai`
- è¿™æ˜¯ **æ—§ç‰ˆ** Google AI SDK (`google-generativeai` åŒ…)
- ä¸é¡¹ç›®ä¸»ä»£ç ä½¿ç”¨çš„ `google-genai` **ä¸åŒ**!
- **é‡è¦åŒºåˆ«**:
  - æ—§ç‰ˆ: `import google.generativeai as genai`
  - æ–°ç‰ˆ: `from google import genai`
- **æ³¨æ„**: æœ¬é¡¹ç›®ä¸»ä»£ç ä½¿ç”¨æ–°ç‰ˆ SDK

**ç¬¬3è¡Œ**: `from dotenv import load_dotenv`
- ä» `.env` æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡

---

```python
# ç¬¬5-6è¡Œ: åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

api_key = os.getenv("GOOGLE_API_KEY")
```

**ç¬¬5è¡Œ**: `load_dotenv()`
- è¯»å–é¡¹ç›®æ ¹ç›®å½•çš„ `.env` æ–‡ä»¶
- å°†é”®å€¼å¯¹åŠ è½½åˆ° `os.environ`

**ç¬¬7è¡Œ**: `api_key = os.getenv("GOOGLE_API_KEY")`
- è¯»å– `GOOGLE_API_KEY` (æ³¨æ„ä¸æ˜¯ `GEMINI_API_KEY`)
- è¿™ä¸ªè„šæœ¬ä½¿ç”¨æ—§çš„å‘½åçº¦å®š

---

```python
# ç¬¬8-10è¡Œ: éªŒè¯å¯†é’¥
if not api_key:
    print("Error: GOOGLE_API_KEY not found.")
    exit(1)
```

**ç¬¬8è¡Œ**: `if not api_key:`
- æ£€æŸ¥å¯†é’¥æ˜¯å¦å­˜åœ¨

**ç¬¬9-10è¡Œ**: é”™è¯¯å¤„ç†
- æ‰“å°é”™è¯¯ä¿¡æ¯
- `exit(1)` - ä»¥é”™è¯¯ä»£ç 1é€€å‡ºç¨‹åº (éé›¶è¡¨ç¤ºé”™è¯¯)

---

```python
# ç¬¬12è¡Œ: é…ç½® SDK
genai.configure(api_key=api_key)
```

**é…ç½®æ–¹å¼å¯¹æ¯”**:

| æ—§ç‰ˆ SDK (æœ¬è„šæœ¬) | æ–°ç‰ˆ SDK (ä¸»ä»£ç ) |
|------------------|------------------|
| `genai.configure(api_key=...)` | `client = genai.Client(api_key=...)` |
| å…¨å±€é…ç½® | å®ä¾‹åŒ–å®¢æˆ·ç«¯ |

---

```python
# ç¬¬14-21è¡Œ: åˆ—å‡ºæ¨¡å‹
print("Listing available models...")
try:
    for m in genai.list_models():
        if 'generateContent' in m.supported_generation_methods:
            print(m.name)
except Exception as e:
    print(f"Error listing models: {e}")
```

**ç¬¬16è¡Œ**: `genai.list_models()`
- è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
- è¿”å›æ¨¡å‹å¯¹è±¡çš„è¿­ä»£å™¨

**ç¬¬17è¡Œ**: è¿‡æ»¤æ¡ä»¶
```python
if 'generateContent' in m.supported_generation_methods:
```
- **ä½œç”¨**: åªæ˜¾ç¤ºæ”¯æŒæ–‡æœ¬ç”Ÿæˆçš„æ¨¡å‹
- **è¿‡æ»¤æ‰**: 
  - åµŒå…¥æ¨¡å‹ (å¦‚ `embedding-001`)
  - å›¾åƒç”Ÿæˆæ¨¡å‹
  - å…¶ä»–ä¸“ç”¨æ¨¡å‹

**ç¬¬18è¡Œ**: `print(m.name)`
- æ‰“å°ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹åç§°

**ç¤ºä¾‹è¾“å‡º**:
```
Listing available models...
models/gemini-1.0-pro
models/gemini-1.0-pro-001
models/gemini-1.0-pro-latest
models/gemini-1.5-pro
models/gemini-1.5-pro-001
models/gemini-1.5-pro-latest
models/gemini-1.5-flash
models/gemini-1.5-flash-001
models/gemini-1.5-flash-latest
models/gemini-2.0-flash-exp
models/gemini-2.5-flash
```

**ç¬¬19-20è¡Œ**: å¼‚å¸¸å¤„ç†
- æ•è·ç½‘ç»œé”™è¯¯ã€è®¤è¯é”™è¯¯ç­‰
- æ‰“å°å‹å¥½çš„é”™è¯¯ä¿¡æ¯

---

### ğŸ“„ `debug_gemini.py` (37è¡Œ)

æµ‹è¯• Gemini å·¥å…·è°ƒç”¨åŠŸèƒ½,éªŒè¯å·¥å…·é…ç½®æ˜¯å¦æ­£ç¡®ã€‚

#### å®Œæ•´ä»£ç é€è¡Œè§£æ

```python
# ç¬¬1-5è¡Œ: å¯¼å…¥
import os
from google import genai
from google.genai import types
from dotenv import load_dotenv
from src.tools import calculator_func, search_func
```

**ç¬¬2-3è¡Œ**: ä½¿ç”¨æ–°ç‰ˆ SDK
```python
from google import genai
from google.genai import types
```
- âœ… **æ­£ç¡®**: ä¸ä¸»é¡¹ç›®ä»£ç ä¸€è‡´
- è¿™æ˜¯ `google-genai` åŒ…çš„å¯¼å…¥æ–¹å¼

**ç¬¬5è¡Œ**: `from src.tools import calculator_func, search_func`
- å¯¼å…¥åŸå§‹å·¥å…·å‡½æ•°
- è¿™äº›å‡½æ•°å°†ä¼ ç»™ Gemini API

---

```python
# ç¬¬7-12è¡Œ: APIå¯†é’¥åŠ è½½
load_dotenv()

api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
if not api_key:
    print("Error: API Key not found")
    exit(1)
```

**ç¬¬9è¡Œ**: ä¼˜é›…çš„å¤‡ç”¨é€»è¾‘
```python
api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
```

**å·¥ä½œåŸç†**:
- Python çš„ **çŸ­è·¯æ±‚å€¼**
- å¦‚æœç¬¬ä¸€ä¸ªå€¼ä¸ºçœŸ â†’ è¿”å›ç¬¬ä¸€ä¸ªå€¼
- å¦‚æœç¬¬ä¸€ä¸ªå€¼ä¸ºå‡ (None) â†’ è¿”å›ç¬¬äºŒä¸ªå€¼
- åŒæ—¶æ”¯æŒä¸¤ç§ç¯å¢ƒå˜é‡å‘½å

**ç­‰ä»·ä»£ç **:
```python
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    api_key = os.getenv("GOOGLE_API_KEY")
```

---

```python
# ç¬¬14-16è¡Œ: åˆå§‹åŒ–å®¢æˆ·ç«¯å’Œå·¥å…·
client = genai.Client(api_key=api_key)

gemini_tools = [calculator_func, search_func]
```

**ç¬¬14è¡Œ**: `client = genai.Client(api_key=api_key)`
- åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹ (æ–°ç‰ˆ SDK)

**ç¬¬16è¡Œ**: `gemini_tools = [calculator_func, search_func]`
- å°†åŸå§‹å‡½æ•°ä¼ ç»™ Gemini
- Gemini ä¼šè‡ªåŠ¨è§£æå‡½æ•°ç­¾åå’Œ docstring

---

```python
# ç¬¬18-37è¡Œ: æµ‹è¯•å·¥å…·è°ƒç”¨
print("Calling Gemini with tool...")
try:
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents="Search for the weather in Timbuktu",
        config=types.GenerateContentConfig(
            tools=gemini_tools
        )
    )
    print("\n--- Response ---")
    print(response)
    print("\n--- Parts ---")
    if response.candidates:
        for part in response.candidates[0].content.parts:
            print(f"Part: {part}")
            if part.function_call:
                print(f"Function Call: {part.function_call}")
except Exception as e:
    print(f"Error: {e}")
```

#### æµ‹è¯•ç”¨ä¾‹åˆ†æ

**ç¬¬22è¡Œ**: æµ‹è¯•è¾“å…¥
```python
contents="Search for the weather in Timbuktu"
```

**ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªè¾“å…¥?**
- åŒ…å«å…³é”®è¯ "Search for"
- åº”è¯¥è§¦å‘ `search_func` å·¥å…·
- Timbuktu (å»·å·´å…‹å›¾) æ˜¯ä¸€ä¸ªçœŸå®åœ°ç‚¹,æµ‹è¯•æœç´¢åŠŸèƒ½

**é¢„æœŸ Gemini è¡Œä¸º**:
1. è¯»å– `search_func` çš„ docstring: "Useful for searching for information on the internet."
2. è¯†åˆ«ç”¨æˆ·æ„å›¾: éœ€è¦æœç´¢ä¿¡æ¯
3. å†³å®šè°ƒç”¨ `search_func(query="weather in Timbuktu")`

---

#### è¾“å‡ºåˆ†æ

**ç¬¬27-28è¡Œ**: æ‰“å°å®Œæ•´å“åº”
```python
print("\n--- Response ---")
print(response)
```

**ç¤ºä¾‹è¾“å‡º**:
```
--- Response ---
GenerateContentResponse(
    candidates=[
        Candidate(
            content=Content(
                role='model',
                parts=[
                    Part(function_call=FunctionCall(
                        name='search_func',
                        args={'query': 'weather in Timbuktu'}
                    ))
                ]
            ),
            finish_reason=FinishReason.STOP
        )
    ]
)
```

---

**ç¬¬29-33è¡Œ**: è¯¦ç»†è§£æ Parts
```python
print("\n--- Parts ---")
if response.candidates:
    for part in response.candidates[0].content.parts:
        print(f"Part: {part}")
        if part.function_call:
            print(f"Function Call: {part.function_call}")
```

**ç¤ºä¾‹è¾“å‡º**:
```
--- Parts ---
Part: function_call {
  name: "search_func"
  args {
    fields {
      key: "query"
      value {
        string_value: "weather in Timbuktu"
      }
    }
  }
}
Function Call: name: "search_func"
args {
  fields {
    key: "query"
    value {
      string_value: "weather in Timbuktu"
    }
  }
}
```

---

#### è°ƒè¯•ä»·å€¼

| è°ƒè¯•ç›®çš„ | éªŒè¯ç‚¹ |
|---------|-------|
| **å·¥å…·è¯†åˆ«** | Gemini æ˜¯å¦æ­£ç¡®è¯†åˆ« `search_func`? |
| **å‚æ•°æå–** | `args` ä¸­çš„ `query` å‚æ•°æ˜¯å¦æ­£ç¡®? |
| **éš”ç¦»æµ‹è¯•** | ä¸æ¶‰åŠ LangGraph,çº¯ API è°ƒç”¨ |
| **é”™è¯¯è¯Šæ–­** | å¦‚æœä¸»ç¨‹åºå‡ºé”™,è¿™é‡Œå¯ä»¥å®šä½æ˜¯å¦æ˜¯ API é—®é¢˜ |

---

## å·¥ä½œæµç¨‹è¯¦è§£

### ğŸ”„ å®Œæ•´å¯¹è¯æµç¨‹

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå…·ä½“ä¾‹å­**æ·±å…¥ç†è§£**æ•´ä¸ªç³»ç»Ÿçš„è¿ä½œ:

**ç”¨æˆ·è¾“å…¥**: "What is 15 * 8?"

---

### é˜¶æ®µ1: ç”¨æˆ·è¾“å…¥å¤„ç†

**æ‰§è¡Œä½ç½®**: `main.py` ç¬¬24è¡Œ

```python
inputs = {"messages": [HumanMessage(content="What is 15 * 8?")]}
```

**åˆå§‹çŠ¶æ€æ„å»º**:
```python
state = {
    "messages": [
        HumanMessage(content="What is 15 * 8?")
    ]
}
```

**çŠ¶æ€ç±»å‹**: `AgentState` (TypedDict)

---

### é˜¶æ®µ2: Agent èŠ‚ç‚¹ - ç¬¬ä¸€æ¬¡è°ƒç”¨

**æ‰§è¡Œ**: `call_model(state)`

#### æ­¥éª¤ 2.1: æ¶ˆæ¯è½¬æ¢

```python
gemini_contents = convert_messages(state['messages'])
```

**è½¬æ¢ç»“æœ**:
```python
[
    types.Content(
        role="user",
        parts=[types.Part.from_text(text="What is 15 * 8?")]
    )
]
```

**å¯è§†åŒ–**:
```
LangChainæ ¼å¼          â†’          Geminiæ ¼å¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
HumanMessage           â†’          Content
â”œâ”€ content: "..."      â†’          â”œâ”€ role: "user"
                                  â””â”€ parts: [Part(text="...")]
```

---

#### æ­¥éª¤ 2.2: è°ƒç”¨ Gemini API

```python
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=gemini_contents,
    config=types.GenerateContentConfig(tools=[calculator_func, search_func])
)
```

**API è¯·æ±‚ç»“æ„**:
```json
{
  "model": "gemini-2.5-flash",
  "contents": [
    {
      "role": "user",
      "parts": [{"text": "What is 15 * 8?"}]
    }
  ],
  "tools": [
    {
      "function_declarations": [
        {
          "name": "calculator_func",
          "description": "Useful for performing mathematical calculations...",
          "parameters": {
            "type": "object",
            "properties": {
              "expression": {"type": "string"}
            }
          }
        },
        {
          "name": "search_func",
          "description": "Useful for searching for information...",
          "parameters": {
            "type": "object", 
            "properties": {
              "query": {"type": "string"}
            }
          }
        }
      ]
    }
  ]
}
```

---

#### æ­¥éª¤ 2.3: Gemini å†…éƒ¨æ¨ç†

**Gemini çš„æ€è€ƒè¿‡ç¨‹** (å†…éƒ¨,ä¸å¯è§):

1. **æ„å›¾è¯†åˆ«**:
   - åˆ†æ: "What is 15 * 8?"
   - è¯†åˆ«: è¿™æ˜¯ä¸€ä¸ªæ•°å­¦è®¡ç®—è¯·æ±‚

2. **å·¥å…·åŒ¹é…**:
   - æ£€æŸ¥ `calculator_func` docstring: "Useful for performing mathematical calculations"
   - âœ… åŒ¹é…åº¦é«˜!
   - æ£€æŸ¥ `search_func` docstring: "Useful for searching for information"
   - âŒ ä¸åŒ¹é…

3. **å†³ç­–**:
   - ä¸ç›´æ¥å›ç­” (è™½ç„¶ Gemini çŸ¥é“ç­”æ¡ˆæ˜¯ 120)
   - é€‰æ‹©è°ƒç”¨å·¥å…·ä»¥å±•ç¤ºå·¥å…·ä½¿ç”¨èƒ½åŠ›
   - æ„é€ å‡½æ•°è°ƒç”¨: `calculator_func(expression="15 * 8")`

---

#### æ­¥éª¤ 2.4: Gemini å“åº”

```python
response.candidates[0].content.parts = [
    Part(function_call=FunctionCall(
        name="calculator_func",
        args={"expression": "15 * 8"}
    ))
]
```

**å“åº”ç»“æ„**:
```
response
â””â”€â”€ candidates: [...]
    â””â”€â”€ [0]
        â”œâ”€â”€ content
        â”‚   â”œâ”€â”€ role: "model"
        â”‚   â””â”€â”€ parts: [Part(function_call=...)]
        â””â”€â”€ finish_reason: STOP
```

---

#### æ­¥éª¤ 2.5: call_model å¤„ç†å“åº”

```python
tool_calls = []
content = ""

for part in response.candidates[0].content.parts:
    if part.function_call:
        fn_name = part.function_call.name  # "calculator_func"
        tool_calls.append({
            "name": fn_name,
            "args": part.function_call.args,  # {"expression": "15 * 8"}
            "id": "call_calculator_func"
        })

return {"messages": [AIMessage(content="", tool_calls=tool_calls)]}
```

**è¿”å›å€¼**:
```python
{
    "messages": [
        AIMessage(
            content="",
            tool_calls=[{
                "name": "calculator_func",
                "args": {"expression": "15 * 8"},
                "id": "call_calculator_func"
            }]
        )
    ]
}
```

---

#### æ­¥éª¤ 2.6: çŠ¶æ€è‡ªåŠ¨åˆå¹¶

**LangGraph è‡ªåŠ¨æ‰§è¡Œ**:
```python
# ç”±äº Annotated[..., operator.add]
state["messages"] = state["messages"] + returned_messages

# ç»“æœ
state = {
    "messages": [
        HumanMessage(content="What is 15 * 8?"),           # åŸæœ‰
        AIMessage(content="", tool_calls=[...])             # æ–°å¢!
    ]
}
```

---

### é˜¶æ®µ3: æ¡ä»¶åˆ¤æ–­ - ç¬¬ä¸€æ¬¡

**æ‰§è¡Œ**: `should_continue(state)`

```python
def should_continue(state: AgentState):
    messages = state['messages']
    last_message = messages[-1]  # AIMessage(tool_calls=[...])
    
    if last_message.tool_calls:  # True! (åˆ—è¡¨éç©º)
        return "tools"
    return END
```

**å†³ç­–**: è¿”å› `"tools"` â†’ è·³è½¬åˆ° `tools` èŠ‚ç‚¹

---

### é˜¶æ®µ4: Tools èŠ‚ç‚¹ - æ‰§è¡Œå·¥å…·

**æ‰§è¡Œ**: `tool_node(state)`

#### ToolNode å†…éƒ¨å·¥ä½œæµç¨‹

**æ­¥éª¤ 4.1**: æå–å·¥å…·è°ƒç”¨
```python
ai_msg = state["messages"][-1]  # AIMessage
tool_call = ai_msg.tool_calls[0]  # {"name": "calculator_func", ...}
```

**æ­¥éª¤ 4.2**: æŸ¥æ‰¾å·¥å…·
```python
tool_name = tool_call["name"]  # "calculator_func"
# åœ¨ langchain_tools ä¸­æŸ¥æ‰¾åä¸º "calculator_func" çš„å·¥å…·
tool = find_tool_by_name(langchain_tools, tool_name)
# æ‰¾åˆ°: calculator å·¥å…·
```

**æ­¥éª¤ 4.3**: æ‰§è¡Œå·¥å…·
```python
# ToolNode è°ƒç”¨
result = tool.invoke(tool_call["args"])

# å†…éƒ¨æµç¨‹
calculator.invoke({"expression": "15 * 8"})
â””â”€> calculator_func("15 * 8")
    â””â”€> str(eval("15 * 8"))
        â””â”€> str(120)
            â””â”€> "120"
```

**æ‰§è¡Œé“¾**:
```
ToolNode
  â†’ calculator (LangChain å·¥å…·)
    â†’ calculator_func (åŸå§‹ Python å‡½æ•°)
      â†’ eval("15 * 8")
        â†’ 120
          â†’ "120" (å­—ç¬¦ä¸²)
```

---

#### æ­¥éª¤ 4.4: æ„é€  ToolMessage

```python
tool_message = ToolMessage(
    name="calculator_func",
    content="120",
    tool_call_id="call_calculator_func"
)
```

**å­—æ®µè¯´æ˜**:
- `name`: å¿…é¡»åŒ¹é…å‡½æ•°å,Gemini éœ€è¦çŸ¥é“è¿™æ˜¯å“ªä¸ªå·¥å…·çš„ç»“æœ
- `content`: å·¥å…·æ‰§è¡Œç»“æœ
- `tool_call_id`: å…³è”åˆ°ä¹‹å‰çš„å·¥å…·è°ƒç”¨

---

#### æ­¥éª¤ 4.5: è¿”å›ç»“æœ

```python
return {"messages": [tool_message]}
```

**çŠ¶æ€æ›´æ–°** (è‡ªåŠ¨åˆå¹¶):
```python
state = {
    "messages": [
        HumanMessage(content="What is 15 * 8?"),
        AIMessage(content="", tool_calls=[...]),
        ToolMessage(name="calculator_func", content="120")  # æ–°å¢!
    ]
}
```

---

### é˜¶æ®µ5: è¿”å› Agent èŠ‚ç‚¹ - ç¬¬äºŒæ¬¡è°ƒç”¨

**å›¾æµè½¬**: `tools` èŠ‚ç‚¹é€šè¿‡å›ºå®šè¾¹è¿”å› `agent` èŠ‚ç‚¹

**æ‰§è¡Œ**: `call_model(state)` (å†æ¬¡è°ƒç”¨)

---

#### æ­¥éª¤ 5.1: æ¶ˆæ¯è½¬æ¢ (å®Œæ•´å†å²)

```python
gemini_contents = convert_messages(state['messages'])
```

**è½¬æ¢ç»“æœ**:
```python
[
    types.Content(
        role="user",
        parts=[Part.from_text(text="What is 15 * 8?")]
    ),
    types.Content(
        role="model",
        parts=[Part.from_function_call(
            name="calculator_func",
            args={"expression": "15 * 8"}
        )]
    ),
    types.Content(
        role="user",  # æ³¨æ„: å·¥å…·ç»“æœçš„ role æ˜¯ "user"!
        parts=[Part.from_function_response(
            name="calculator_func",
            response={"result": "120"}
        )]
    )
]
```

**å…³é”®ç‚¹**: å®Œæ•´çš„ä¸‰è½®å¯¹è¯å†å²!

---

#### æ­¥éª¤ 5.2: è°ƒç”¨ Gemini (å¸¦å†å²)

```python
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=gemini_contents,  # å®Œæ•´å†å²!
    config=types.GenerateContentConfig(tools=gemini_tools)
)
```

**Gemini çœ‹åˆ°çš„ä¸Šä¸‹æ–‡**:
1. ç”¨æˆ·é—®: "What is 15 * 8?"
2. æˆ‘ (Gemini) è°ƒç”¨äº†: `calculator_func("15 * 8")`
3. å·¥å…·è¿”å›äº†: "120"
4. ç°åœ¨æˆ‘éœ€è¦: ç»™ç”¨æˆ·ä¸€ä¸ªè‡ªç„¶è¯­è¨€å›å¤

---

#### æ­¥éª¤ 5.3: Gemini å“åº” (æœ€ç»ˆç­”æ¡ˆ)

```python
response.candidates[0].content.parts = [
    Part(text="15 multiplied by 8 equals 120.")
]
```

**æ³¨æ„**: è¿™æ¬¡æ²¡æœ‰ `function_call`! åªæœ‰æ–‡æœ¬ã€‚

---

#### æ­¥éª¤ 5.4: call_model å¤„ç†

```python
tool_calls = []  # ç©ºåˆ—è¡¨!
content = "15 multiplied by 8 equals 120."

return {"messages": [AIMessage(content=content, tool_calls=[])]}
```

**çŠ¶æ€æ›´æ–°**:
```python
state = {
    "messages": [
        HumanMessage(content="What is 15 * 8?"),
        AIMessage(content="", tool_calls=[...]),
        ToolMessage(name="calculator_func", content="120"),
        AIMessage(content="15 multiplied by 8 equals 120.", tool_calls=[])  # æ–°å¢!
    ]
}
```

---

### é˜¶æ®µ6: æ¡ä»¶åˆ¤æ–­ - ç¬¬äºŒæ¬¡

**æ‰§è¡Œ**: `should_continue(state)`

```python
last_message = state["messages"][-1]
# AIMessage(content="...", tool_calls=[])

if last_message.tool_calls:  # False! (ç©ºåˆ—è¡¨)
    return "tools"
return END  # æ‰§è¡Œè¿™é‡Œ!
```

**å†³ç­–**: è¿”å› `END` â†’ å¯¹è¯ç»“æŸ

---

### é˜¶æ®µ7: ç”¨æˆ·ç•Œé¢è¾“å‡º

**main.py çš„æµå¼è¾“å‡º**:

```
You: What is 15 * 8?
Agent is thinking...
Node 'agent':
Calling Tool: calculator_func
---
Node 'tools':
---
Node 'agent':
15 multiplied by 8 equals 120.
---
--------------------------------------------------
```

**è¾“å‡ºè§£æ**:
1. **Node 'agent'**: ç¬¬ä¸€æ¬¡è°ƒç”¨,å†³å®šä½¿ç”¨å·¥å…·
2. **Node 'tools'**: æ‰§è¡Œå·¥å…· (æ— å¯è§è¾“å‡º)
3. **Node 'agent'**: ç¬¬äºŒæ¬¡è°ƒç”¨,æä¾›æœ€ç»ˆç­”æ¡ˆ

---

### ğŸ“Š å®Œæ•´æµç¨‹å¯è§†åŒ–

```mermaid
sequenceDiagram
    autonumber
    participant U as ç”¨æˆ·
    participant M as main.py
    participant A as AgentèŠ‚ç‚¹<br/>(call_model)
    participant G as Gemini API
    participant SC as should_continue
    participant T as ToolsèŠ‚ç‚¹<br/>(ToolNode)
    participant C as calculator_func

    U->>M: "What is 15 * 8?"
    M->>A: state: [HumanMessage]
    
    Note over A: ç¬¬ä¸€æ¬¡è°ƒç”¨
    A->>A: convert_messages()
    A->>G: generate_content(history, tools)
    
    Note over G: åˆ†æ: éœ€è¦è®¡ç®—<br/>å†³å®š: è°ƒç”¨ calculator_func
    G-->>A: FunctionCall(calculator_func)
    
    A-->>M: AIMessage(tool_calls=[...])
    M->>SC: æ£€æŸ¥æœ€åæ¶ˆæ¯
    
    Note over SC: æœ‰tool_calls â†’ "tools"
    SC-->>M: è¿”å› "tools"
    
    M->>T: state: [..., AIMessage]
    T->>T: æå– tool_call
    T->>C: calculator_func("15 * 8")
    C-->>T: "120"
    T-->>M: ToolMessage(content="120")
    
    Note over M: å›ºå®šè¾¹ â†’ è¿”å› agent
    
    M->>A: state: [..., ToolMessage]
    
    Note over A: ç¬¬äºŒæ¬¡è°ƒç”¨
    A->>A: convert_messages()<br/>(å®Œæ•´å†å²)
    A->>G: generate_content(å®Œæ•´å†å²)
    
    Note over G: è¯»å–å·¥å…·ç»“æœ<br/>ç”Ÿæˆè‡ªç„¶è¯­è¨€ç­”æ¡ˆ
    G-->>A: Part(text="15 multiplied...")
    
    A-->>M: AIMessage(content="...", tool_calls=[])
    M->>SC: æ£€æŸ¥æœ€åæ¶ˆæ¯
    
    Note over SC: æ— tool_calls â†’ END
    SC-->>M: è¿”å› END
    
    M->>U: "15 multiplied by 8 equals 120."
```

---

### ğŸ§  çŠ¶æ€æ¼”å˜æ—¶é—´çº¿

| æ—¶é—´ | èŠ‚ç‚¹ | æ“ä½œ | messages åˆ—è¡¨å†…å®¹ |
|-----|------|------|------------------|
| T0 | - | åˆå§‹åŒ– | `[]` |
| T1 | main | ç”¨æˆ·è¾“å…¥ | `[HumanMessage("What is 15 * 8?")]` |
| T2 | agent | ç¬¬ä¸€æ¬¡è¿”å› | `+ [AIMessage(tool_calls=[...])]` |
| T3 | tools | æ‰§è¡Œå·¥å…· | `+ [ToolMessage(content="120")]` |
| T4 | agent | ç¬¬äºŒæ¬¡è¿”å› | `+ [AIMessage(content="15 multiplied...")]` |
| T5 | - | ç»“æŸ | (ä¸å˜,å…±4æ¡æ¶ˆæ¯) |

---

### ğŸ“ˆ æ•°æ®æµå›¾

```mermaid
graph TD
    Start[ç”¨æˆ·è¾“å…¥] -->|HumanMessage| State1[State: 1æ¡æ¶ˆæ¯]
    State1 -->|agentèŠ‚ç‚¹| Convert1[convert_messages]
    Convert1 --> GeminiCall1[Gemini API è°ƒç”¨ #1]
    GeminiCall1 -->|FunctionCall| State2[State: 2æ¡æ¶ˆæ¯]
    
    State2 -->|should_continue<br/>â†’ tools| ToolNode[ToolNode]
    ToolNode -->|æ‰§è¡Œcalculator_func| Result[ç»“æœ: 120]
    Result --> State3[State: 3æ¡æ¶ˆæ¯]
    
    State3 -->|å›ºå®šè¾¹<br/>â†’ agent| Convert2[convert_messages<br/>å®Œæ•´å†å²]
    Convert2 --> GeminiCall2[Gemini API è°ƒç”¨ #2]
    GeminiCall2 -->|Textå“åº”| State4[State: 4æ¡æ¶ˆæ¯]
    
    State4 -->|should_continue<br/>â†’ END| Output[è¾“å‡ºç»“æœ]
    
    style State1 fill:#e1f5ff
    style State2 fill:#e1f5ff
    style State3 fill:#e1f5ff
    style State4 fill:#e1f5ff
    style GeminiCall1 fill:#fff4e1
    style GeminiCall2 fill:#fff4e1
    style ToolNode fill:#e8f5e9
```

---

## æœ€ä½³å®è·µ

### âœ… ç”Ÿäº§ç¯å¢ƒå»ºè®®

#### 1. å®‰å…¨æ€§å¢å¼º

##### é—®é¢˜: eval() çš„å±é™©æ€§

**âŒ å½“å‰ä»£ç  (tools.py ç¬¬12è¡Œ)**:
```python
return str(eval(expression))  # å±é™©!
```

**ä¸ºä»€ä¹ˆå±é™©?**
```python
# æ¶æ„è¾“å…¥ç¤ºä¾‹
calculator_func("__import__('os').system('rm -rf /')")
# è¿™ä¼šåˆ é™¤ç³»ç»Ÿæ–‡ä»¶!

calculator_func("open('/etc/passwd').read()")
# è¿™ä¼šè¯»å–æ•æ„Ÿæ–‡ä»¶!
```

---

##### è§£å†³æ–¹æ¡ˆ1: ä½¿ç”¨ AST (æ¨è)

```python
import ast
import operator as op

# æ”¯æŒçš„æ“ä½œç¬¦
operators = {
    ast.Add: op.add,       # +
    ast.Sub: op.sub,       # -
    ast.Mult: op.mul,      # *
    ast.Div: op.truediv,   # /
    ast.Pow: op.pow,       # **
    ast.Mod: op.mod,       # %
    ast.FloorDiv: op.floordiv,  # //
    ast.USub: op.neg,      # -x (ä¸€å…ƒè´Ÿå·)
}

def safe_eval(node):
    """é€’å½’æ±‚å€¼ AST èŠ‚ç‚¹"""
    if isinstance(node, ast.Num):  # æ•°å­—
        return node.n
    elif isinstance(node, ast.BinOp):  # äºŒå…ƒæ“ä½œ
        left = safe_eval(node.left)
        right = safe_eval(node.right)
        return operators[type(node.op)](left, right)
    elif isinstance(node, ast.UnaryOp):  # ä¸€å…ƒæ“ä½œ
        operand = safe_eval(node.operand)
        return operators[type(node.op)](operand)
    else:
        raise ValueError(f"Unsupported operation: {type(node)}")

def calculator_func(expression: str) -> str:
    """
    å®‰å…¨çš„æ•°å­¦è®¡ç®—å™¨,åªæ”¯æŒåŸºæœ¬ç®—æœ¯è¿ç®—ã€‚
    æ”¯æŒ: +, -, *, /, **, %, //
    ç¤ºä¾‹: "5 + 5", "10 * 2", "2 ** 3"
    """
    try:
        # è§£æè¡¨è¾¾å¼
        tree = ast.parse(expression, mode='eval')
        # æ±‚å€¼
        result = safe_eval(tree.body)
        return str(result)
    except (ValueError, KeyError, SyntaxError) as e:
        return f"Error: Invalid expression - {e}"
    except ZeroDivisionError:
        return "Error: Division by zero"
    except Exception as e:
        return f"Error calculating: {e}"
```

**æµ‹è¯•**:
```python
print(calculator_func("2 + 3"))        # "5"
print(calculator_func("10 * 5"))       # "50"
print(calculator_func("2 ** 3"))       # "8"
print(calculator_func("10 / 2"))       # "5.0"
print(calculator_func("os.system('')")) # "Error: Unsupported operation"
```

---

##### è§£å†³æ–¹æ¡ˆ2: ä½¿ç”¨ simpleeval åº“

```bash
pip install simpleeval
```

```python
from simpleeval import simple_eval

def calculator_func(expression: str) -> str:
    """
    ä½¿ç”¨ simpleeval è¿›è¡Œå®‰å…¨è®¡ç®—ã€‚
    æ”¯æŒåŸºæœ¬æ•°å­¦è¿ç®—å’Œå¸¸è§å‡½æ•° (sin, cos, sqrt ç­‰)ã€‚
    """
    try:
        result = simple_eval(expression)
        return str(result)
    except Exception as e:
        return f"Error: {e}"
```

**ä¼˜ç‚¹**:
- è‡ªåŠ¨æ”¯æŒæ•°å­¦å‡½æ•°
- å¯è‡ªå®šä¹‰å…è®¸çš„å‡½æ•°å’Œå˜é‡
- æ€§èƒ½ä¼˜ç§€

---

##### è§£å†³æ–¹æ¡ˆ3: ä½¿ç”¨ sympy (é«˜çº§æ•°å­¦)

```bash
pip install sympy
```

```python
from sympy import sympify, SympifyError
from sympy.parsing.sympy_parser import parse_expr

def calculator_func(expression: str) -> str:
    """
    ä½¿ç”¨ SymPy è¿›è¡Œç¬¦å·è®¡ç®—ã€‚
    æ”¯æŒå¤æ‚æ•°å­¦è¡¨è¾¾å¼å’Œç¬¦å·è¿ç®—ã€‚
    """
    try:
        result = sympify(expression)
        # å¦‚æœæ˜¯æ•°å€¼è¡¨è¾¾å¼,æ±‚å€¼
        if result.is_number:
            return str(float(result))
        # å¦åˆ™è¿”å›ç¬¦å·è¡¨è¾¾å¼
        return str(result)
    except (SympifyError, ValueError) as e:
        return f"Error: Invalid expression - {e}"
```

---

#### 2. ç¯å¢ƒå˜é‡ç®¡ç†

**âŒ ä¸å®‰å…¨åšæ³•**:
```python
# .env æ–‡ä»¶æäº¤åˆ° Git
GEMINI_API_KEY=AIzaSyCH257-Hn30gQOKDfHVwGgPICZ5IjO8DCU
```

**âœ… å®‰å…¨åšæ³•**:

##### æ­¥éª¤1: .gitignore
```bash
# .gitignore
.env
.env.local
.env.*.local
*.key
```

##### æ­¥éª¤2: ç¯å¢ƒå˜é‡æ¨¡æ¿
```bash
# .env.example (æäº¤åˆ° Git)
GEMINI_API_KEY=your-api-key-here
GOOGLE_API_KEY=your-api-key-here

# è¯´æ˜
# ä» https://makersuite.google.com/app/apikey è·å– API å¯†é’¥
```

##### æ­¥éª¤3: æ–‡æ¡£è¯´æ˜
```markdown
# è®¾ç½®æ­¥éª¤

1. å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿:
   ```bash
   cp .env.example .env
   ```

2. ç¼–è¾‘ `.env` æ–‡ä»¶,å¡«å…¥ä½ çš„ API å¯†é’¥

3. è¿è¡Œç¨‹åº:
   ```bash
   python main.py
   ```
```

---

##### ç”Ÿäº§ç¯å¢ƒ: ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡

**Google Secret Manager**:
```python
from google.cloud import secretmanager

def get_api_key(project_id: str, secret_id: str) -> str:
    """ä» Google Secret Manager è·å–å¯†é’¥"""
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/latest"
    response = client.access_secret_version(request={"name": name})
    return response.payload.data.decode("UTF-8")

# ä½¿ç”¨
api_key = get_api_key("my-project", "gemini-api-key")
client = genai.Client(api_key=api_key)
```

**AWS Secrets Manager**:
```python
import boto3
import json

def get_api_key(secret_name: str, region_name: str = "us-east-1") -> str:
    """ä» AWS Secrets Manager è·å–å¯†é’¥"""
    session = boto3.session.Session()
    client = session.client(
        service_name='secretsmanager',
        region_name=region_name
    )
    response = client.get_secret_value(SecretId=secret_name)
    secret = json.loads(response['SecretString'])
    return secret['GEMINI_API_KEY']
```

---

#### 3. é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

##### æ·»åŠ é‡è¯•é€»è¾‘

```bash
pip install tenacity
```

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
from google.api_core import exceptions as google_exceptions

@retry(
    # æœ€å¤šé‡è¯•3æ¬¡
    stop=stop_after_attempt(3),
    # æŒ‡æ•°é€€é¿: 4ç§’, 8ç§’, 16ç§’
    wait=wait_exponential(multiplier=1, min=4, max=60),
    # åªé‡è¯•ç‰¹å®šå¼‚å¸¸
    retry=retry_if_exception_type((
        google_exceptions.ServiceUnavailable,
        google_exceptions.DeadlineExceeded,
        google_exceptions.ResourceExhausted
    ))
)
def call_model(state: AgentState):
    """è°ƒç”¨ Gemini æ¨¡å‹,è‡ªåŠ¨é‡è¯•"""
    logger.info(f"Calling Gemini with {len(state['messages'])} messages")
    
    try:
        messages = state['messages']
        gemini_contents = convert_messages(messages)
        
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=gemini_contents,
            config=types.GenerateContentConfig(tools=gemini_tools)
        )
        
        logger.info("Gemini responded successfully")
        # ... å¤„ç†å“åº”
        
    except google_exceptions.InvalidArgument as e:
        # ä¸é‡è¯•çš„é”™è¯¯
        logger.error(f"Invalid argument: {e}")
        raise
    except Exception as e:
        logger.warning(f"API call failed (will retry): {e}")
        raise
```

---

##### è¯¦ç»†çš„å¼‚å¸¸å¤„ç†

```python
def call_model(state: AgentState):
    try:
        # ... API è°ƒç”¨
        pass
    except google_exceptions.PermissionDenied:
        # API å¯†é’¥æ— æ•ˆ
        return {"messages": [AIMessage(
            content="Error: API å¯†é’¥æ— æ•ˆ,è¯·æ£€æŸ¥é…ç½®"
        )]}
    except google_exceptions.ResourceExhausted:
        # é…é¢ç”¨å°½
        return {"messages": [AIMessage(
            content="Error: API é…é¢å·²ç”¨å°½,è¯·ç¨åå†è¯•"
        )]}
    except google_exceptions.InvalidArgument as e:
        # è¯·æ±‚å‚æ•°é”™è¯¯
        logger.error(f"Invalid request: {e}")
        return {"messages": [AIMessage(
            content=f"Error: è¯·æ±‚å‚æ•°é”™è¯¯ - {e}"
        )]}
    except Exception as e:
        # å…¶ä»–æœªçŸ¥é”™è¯¯
        logger.exception("Unexpected error in call_model")
        return {"messages": [AIMessage(
            content=f"Error: å‘ç”ŸæœªçŸ¥é”™è¯¯ - {e}"
        )]}
```

---

#### 4. æ—¥å¿—è®°å½•

##### é…ç½®ç»“æ„åŒ–æ—¥å¿—

```python
import logging
import sys
from pathlib import Path

# åˆ›å»ºæ—¥å¿—ç›®å½•
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        # æ–‡ä»¶å¤„ç†å™¨
        logging.FileHandler(log_dir / 'agent.log', encoding='utf-8'),
        # æ§åˆ¶å°å¤„ç†å™¨
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getlogger(__name__)
```

##### åœ¨ä»£ç ä¸­ä½¿ç”¨æ—¥å¿—

```python
def call_model(state: AgentState):
    logger.info("=" * 50)
    logger.info(f"Calling Gemini model")
    logger.debug(f"Current state has {len(state['messages'])} messages")
    
    # è®°å½•æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    user_msgs = [m for m in state['messages'] if isinstance(m, HumanMessage)]
    if user_msgs:
        logger.info(f"User query: {user_msgs[-1].content[:100]}")
    
    start_time = time.time()
    
    try:
        response = client.models.generate_content(...)
        
        elapsed = time.time() - start_time
        logger.info(f"API call succeeded in {elapsed:.2f}s")
        
        # è®°å½•å“åº”ç±»å‹
        if response.candidates:
            parts = response.candidates[0].content.parts
            has_text = any(p.text for p in parts)
            has_tool_call = any(p.function_call for p in parts)
            
            if has_tool_call:
                tool_names = [p.function_call.name for p in parts if p.function_call]
                logger.info(f"Model requested tools: {tool_names}")
            elif has_text:
                logger.info("Model provided text response")
        
        return {"messages": [AIMessage(...)]}
        
    except Exception as e:
        logger.error(f"Error in call_model: {e}", exc_info=True)
        raise
```

---

#### 5. æ€§èƒ½ä¼˜åŒ–

##### Token ä½¿ç”¨ç›‘æ§

```python
def call_model(state: AgentState):
    response = client.models.generate_content(...)
    
    # è®°å½•tokenä½¿ç”¨æƒ…å†µ
    if hasattr(response, 'usage_metadata'):
        usage = response.usage_metadata
        logger.info(
            f"Token usage - "
            f"Prompt: {usage.prompt_token_count}, "
            f"Response: {usage.candidates_token_count}, "
            f"Total: {usage.total_token_count}"
        )
        
        # è­¦å‘Š: token ä½¿ç”¨è¿‡å¤š
        if usage.total_token_count > 30000:
            logger.warning("Token usage is high! Consider truncating history")
    
    return {"messages": [AIMessage(...)]}
```

---

##### å†å²æ¶ˆæ¯æˆªæ–­

```python
def truncate_messages(messages: list[BaseMessage], max_messages: int = 20) -> list[BaseMessage]:
    """ä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯,é¿å…ä¸Šä¸‹æ–‡è¿‡é•¿"""
    if len(messages) <= max_messages:
        return messages
    
    # ä¿ç•™ç¬¬ä¸€æ¡ (é€šå¸¸æ˜¯ç³»ç»Ÿæç¤º) å’Œæœ€è¿‘çš„æ¶ˆæ¯
    return [messages[0]] + messages[-(max_messages-1):]

def call_model(state: AgentState):
    messages = state['messages']
    
    # æˆªæ–­å†å²
    messages = truncate_messages(messages, max_messages=20)
    
    gemini_contents = convert_messages(messages)
    # ...
```

---

##### ç¼“å­˜é‡å¤è½¬æ¢

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def convert_single_message_cached(msg_type: str, content: str, role: str) -> types.Content:
    """ç¼“å­˜å•ä¸ªæ¶ˆæ¯çš„è½¬æ¢ç»“æœ"""
    if msg_type == "human":
        return types.Content(
            role="user",
            parts=[types.Part.from_text(text=content)]
        )
    # ... å…¶ä»–ç±»å‹
```

---

#### 6. å·¥å…·æ‰©å±•ç¤ºä¾‹

##### çœŸå®æœç´¢å·¥å…· (DuckDuckGo)

```bash
pip install duckduckgo-search
```

```python
from duckduckgo_search import DDGS

def search_func(query: str) -> str:
    """
    Search for information on the internet using DuckDuckGo.
    Returns a summary of the top 3 search results.
    
    Args:
        query: The search query string
        
    Returns:
        A formatted string with search results
    """
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=3))
            
        if not results:
            return f"No search results found for: {query}"
        
        summary = f"Search results for '{query}':\n\n"
        for i, result in enumerate(results, 1):
            summary += f"{i}. {result['title']}\n"
            summary += f"   {result['body'][:200]}...\n"
            summary += f"   Source: {result['href']}\n\n"
        
        return summary
        
    except Exception as e:
        return f"Search error: {e}"

# æ›´æ–° LangChain å·¥å…·
search = tool("search_func")(search_func)
```

---

##### æ–‡ä»¶æ“ä½œå·¥å…·

```python
from pathlib import Path

def read_file_func(filepath: str) -> str:
    """
    Read and return the contents of a text file.
    
    Args:
        filepath: Absolute or relative path to the file
        
    Returns:
        File contents as a string (max 5000 chars)
    """
    try:
        file_path = Path(filepath)
        
        # å®‰å…¨æ£€æŸ¥
        if not file_path.exists():
            return f"Error: File not found: {filepath}"
        
        if not file_path.is_file():
            return f"Error: Not a file: {filepath}"
        
        # é™åˆ¶æ–‡ä»¶å¤§å°
        if file_path.stat().st_size > 1_000_000:  # 1MB
            return "Error: File too large (max 1MB)"
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read(5000)  # é™åˆ¶å­—ç¬¦æ•°
        
        return content
        
    except UnicodeDecodeError:
        return f"Error: Cannot decode file (not a text file?)"
    except PermissionError:
        return f"Error: Permission denied: {filepath}"
    except Exception as e:
        return f"Error reading file: {e}"

# åˆ›å»ºå·¥å…·
read_file = tool("read_file_func")(read_file_func)

# æ³¨å†Œ
gemini_tools = [calculator_func, search_func, read_file_func]
langchain_tools = [calculator, search, read_file]
```

---

##### å¤©æ°”æŸ¥è¯¢å·¥å…·

```bash
pip install requests
```

```python
import requests

def get_weather_func(city: str) -> str:
    """
    Get current weather information for a city.
    Uses OpenWeatherMap API (free tier).
    
    Args:
        city: City name (e.g., "London", "Tokyo")
        
    Returns:
        Weather description and temperature
    """
    try:
        # æ³¨æ„: éœ€è¦åœ¨ .env ä¸­æ·»åŠ  OPENWEATHERMAP_API_KEY
        api_key = os.getenv("OPENWEATHERMAP_API_KEY")
        if not api_key:
            return "Error: OpenWeatherMap API key not configured"
        
        url = f"http://api.openweathermap.org/data/2.5/weather"
        params = {
            "q": city,
            "appid": api_key,
            "units": "metric"  # æ‘„æ°åº¦
        }
        
        response = requests.get(url, params=params, timeout=5)
        response.raise_for_status()
        
        data = response.json()
        
        temp = data["main"]["temp"]
        feels_like = data["main"]["feels_like"]
        description = data["weather"][0]["description"]
        humidity = data["main"]["humidity"]
        
        return (
            f"Weather in {city}:\n"
            f"Temperature: {temp}Â°C (feels like {feels_like}Â°C)\n"
            f"Conditions: {description}\n"
            f"Humidity: {humidity}%"
        )
        
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            return f"Error: City not found: {city}"
        return f"API error: {e}"
    except requests.exceptions.Timeout:
        return "Error: Request timed out"
    except Exception as e:
        return f"Error: {e}"

get_weather = tool("get_weather_func")(get_weather_func)
```

---

#### 7. æŒä¹…åŒ–å­˜å‚¨

**å½“å‰é—®é¢˜**: `MemorySaver` åªä¿å­˜åœ¨å†…å­˜ä¸­,ç¨‹åºé‡å¯åå¯¹è¯å†å²ä¸¢å¤±ã€‚

##### è§£å†³æ–¹æ¡ˆ1: SQLite (æœ¬åœ°)

```bash
pip install aiosqlite
```

```python
from langgraph.checkpoint.sqlite import SqliteSaver

# åˆ›å»º SQLite æ£€æŸ¥ç‚¹å™¨
checkpointer = SqliteSaver.from_conn_string("agent_checkpoints.db")

# ç¼–è¯‘å›¾
app = workflow.compile(checkpointer=checkpointer)
```

**ä¼˜ç‚¹**:
- æœ¬åœ°æ–‡ä»¶æ•°æ®åº“
- æ— éœ€é¢å¤–æœåŠ¡
- è‡ªåŠ¨ä¿å­˜æ‰€æœ‰çŠ¶æ€

**æ•°æ®åº“ç»“æ„**:
```sql
-- checkpoints è¡¨
CREATE TABLE checkpoints (
    thread_id TEXT,
    checkpoint_id TEXT,
    parent_id TEXT,
    data BLOB,
    PRIMARY KEY (thread_id, checkpoint_id)
);
```

---

##### è§£å†³æ–¹æ¡ˆ2: PostgreSQL (ç”Ÿäº§)

```bash
pip install psycopg2-binary
```

```python
from langgraph.checkpoint.postgres import PostgresSaver

# è¿æ¥ PostgreSQL
checkpointer = PostgresSaver.from_conn_string(
    "postgresql://user:password@localhost:5432/agentdb"
)

app = workflow.compile(checkpointer=checkpointer)
```

**ä¼˜ç‚¹**:
- ç”Ÿäº§çº§æ•°æ®åº“
- æ”¯æŒå¹¶å‘
- å¯æ‰©å±•

---

##### æŸ¥è¯¢å†å²å¯¹è¯

```python
# è·å–æŸä¸ªçº¿ç¨‹çš„æ‰€æœ‰æ£€æŸ¥ç‚¹
checkpoints = checkpointer.list({"configurable": {"thread_id": "1"}})

for cp in checkpoints:
    print(f"Checkpoint: {cp.id}")
    print(f"Messages: {len(cp.data['messages'])}")
```

---

#### 8. å¤šä¼šè¯ç®¡ç†

```python
import uuid

def main():
    print("Multi-session Agent")
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆå”¯ä¸€ thread_id
    user_id = input("Enter your user ID (or press Enter for new): ")
    
    if not user_id:
        user_id = str(uuid.uuid4())
        print(f"New session created: {user_id}")
    
    config = {"configurable": {"thread_id": user_id}}
    
    # ... å¯¹è¯å¾ªç¯
```

---

#### 9. æµå¼å“åº”

**å½“å‰**: ç­‰å¾…å®Œæ•´å“åº”åæ‰æ˜¾ç¤º

**æ”¹è¿›**: é€å­—è¾“å‡º (æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ)

```python
# æ³¨æ„: éœ€è¦ Gemini API æ”¯æŒæµå¼å“åº”
def call_model_streaming(state: AgentState):
    """æµå¼è°ƒç”¨ Gemini"""
    messages = state['messages']
    gemini_contents = convert_messages(messages)
    
    # æµå¼è¯·æ±‚
    stream = client.models.generate_content_stream(
        model="gemini-2.5-flash",
        contents=gemini_contents,
        config=types.GenerateContentConfig(tools=gemini_tools)
    )
    
    full_content = ""
    tool_calls = []
    
    for chunk in stream:
        if chunk.candidates:
            for part in chunk.candidates[0].content.parts:
                if part.text:
                    # é€å­—æ‰“å°
                    print(part.text, end="", flush=True)
                    full_content += part.text
                if part.function_call:
                    tool_calls.append({
                        "name": part.function_call.name,
                        "args": part.function_call.args,
                        "id": f"call_{part.function_call.name}"
                    })
    
    print()  # æ¢è¡Œ
    return {"messages": [AIMessage(content=full_content, tool_calls=tool_calls)]}
```

---

#### 10. Web ç•Œé¢

##### ä½¿ç”¨ Streamlit

```bash
pip install streamlit
```

```python
# webapp.py
import streamlit as st
from langchain_core.messages import HumanMessage
from src.graph import app

st.title("ğŸ¤– LangGraph Gemini Agent")

# ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "thread_id" not in st.session_state:
    st.session_state.thread_id = str(uuid.uuid4())

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("Your message"):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # è°ƒç”¨ä»£ç†
    config = {"configurable": {"thread_id": st.session_state.thread_id}}
    inputs = {"messages": [HumanMessage(content=prompt)]}
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        for output in app.stream(inputs, config=config):
            for key, value in output.items():
                if "messages" in value and value["messages"]:
                    last_msg = value["messages"][-1]
                    if hasattr(last_msg, "content") and last_msg.content:
                        full_response = last_msg.content
                        message_placeholder.markdown(full_response)
        
        st.session_state.messages.append({"role": "assistant", "content": full_response})
```

**è¿è¡Œ**:
```bash
streamlit run webapp.py
```

---

## è¿›é˜¶è¯é¢˜

### ğŸ¤– å¤šä»£ç†ç³»ç»Ÿ

åˆ›å»ºä¸“é—¨çš„ä»£ç†å¤„ç†ä¸åŒä»»åŠ¡:

```python
from langgraph.graph import StateGraph

# å®šä¹‰ä¸“é—¨çš„ä»£ç†
def create_math_agent():
    """æ•°å­¦ä¸“å®¶ä»£ç†"""
    workflow = StateGraph(AgentState)
    workflow.add_node("agent", call_model)
    workflow.add_node("tools", ToolNode([calculator]))
    # ... é…ç½®
    return workflow.compile()

def create_search_agent():
    """æœç´¢ä¸“å®¶ä»£ç†"""
    workflow = StateGraph(AgentState)
    workflow.add_node("agent", call_model)
    workflow.add_node("tools", ToolNode([search]))
    # ... é…ç½®
    return workflow.compile()

# ç›‘ç£è€…ä»£ç†
def supervisor_node(state: AgentState):
    """å†³å®šè°ƒç”¨å“ªä¸ªä¸“å®¶ä»£ç†"""
    last_message = state["messages"][-1].content.lower()
    
    if any(word in last_message for word in ["calculate", "math", "compute"]):
        return "math_agent"
   elif any(word in last_message for word in ["search", "find", "look up"]):
        return "search_agent"
    else:
        return "general_agent"

# ä¸»å›¾
workflow = StateGraph(AgentState)
workflow.add_node("supervisor", supervisor_node)
workflow.add_node("math_agent", create_math_agent())
workflow.add_node("search_agent", create_search_agent())
# ... é“¾æ¥èŠ‚ç‚¹
```

---

### ğŸ“Š å¯è§†åŒ–å·¥å…·

##### ç»˜åˆ¶å›¾ç»“æ„

```python
from IPython.display import Image, display

# ç”Ÿæˆå›¾çš„å¯è§†åŒ–
display(Image(app.get_graph().draw_mermaid_png()))
```

##### ç”Ÿæˆ Mermaid ä»£ç 

```python
mermaid_code = app.get_graph().draw_mermaid()
print(mermaid_code)
```

**è¾“å‡º**:
```mermaid
%%{init: {'flowchart': {'curve': 'linear'}}}%%
graph TD;
	__start__([<p>__start__</p>]):::first
	agent(agent)
	tools(tools)
	__end__([<p>__end__</p>]):::last
	__start__ --> agent;
	tools --> agent;
	agent -.->|tools| tools;
	agent -.->|__end__| __end__;
	classDef default fill:#f2f0ff,line-height:1.2
	classDef first fill-opacity:0
	classDef last fill:#bfb6fc
```

---

### ğŸ“ å­¦ä¹ èµ„æº

1. **LangGraph å®˜æ–¹æ–‡æ¡£**: [https://langchain-ai.github.io/langgraph/](https://langchain-ai.github.io/langgraph/)
2. **Gemini API æ–‡æ¡£**: [https://ai.google.dev/docs](https://ai.google.dev/docs)
3. **LangChain å·¥å…·æŒ‡å—**: [https://python.langchain.com/docs/modules/agents/tools/](https://python.langchain.com/docs/modules/agents/tools/)
4. **Google GenAI SDK**: [https://github.com/google/generative-ai-python](https://github.com/google/generative-ai-python)

---

## æ€»ç»“

### ğŸ¯ é¡¹ç›®æ ¸å¿ƒè®¾è®¡

æœ¬é¡¹ç›®å±•ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„ã€ç”Ÿäº§çº§çš„ AI Agent æ¶æ„:

#### 1. æ¨¡å—åŒ–è®¾è®¡
- **å·¥å…·å±‚** (`tools.py`): å®šä¹‰å¯å¤ç”¨çš„åŠŸèƒ½
- **å›¾é€»è¾‘** (`graph.py`): ç¼–æ’çŠ¶æ€æµè½¬
- **ä¸»ç¨‹åº** (`main.py`): ç”¨æˆ·äº¤äº’ç•Œé¢

#### 2. æœ‰çŠ¶æ€å¯¹è¯
- ä½¿ç”¨ `Annotated[..., operator.add]` è‡ªåŠ¨ç´¯ç§¯å†å²
- LangGraph çš„æ£€æŸ¥ç‚¹æœºåˆ¶æ”¯æŒæŒä¹…åŒ–

#### 3. å·¥å…·é›†æˆ
- åŒé‡å®šä¹‰: Gemini å‡½æ•° + LangChain å·¥å…·
- æ— ç¼çš„å‡½æ•°è°ƒç”¨å’Œç»“æœå¤„ç†

#### 4. å¯æ‰©å±•æ€§
- æ˜“äºæ·»åŠ æ–°å·¥å…· (åªéœ€ 3 æ­¥)
- æ”¯æŒå¤šèŠ‚ç‚¹ã€å¤šä»£ç†æ¶æ„

---

### â­ å…³é”®è®¾è®¡äº®ç‚¹

| è®¾è®¡ | ä½œç”¨ |
|-----|------|
| **åŒå·¥å…·åˆ—è¡¨** | å…¼å®¹ Gemini å’Œ LangChain |
| **æ¶ˆæ¯è½¬æ¢å±‚** | ä¼˜é›…å¤„ç†æ ¼å¼å·®å¼‚ |
| **æ¡ä»¶è·¯ç”±** | è‡ªåŠ¨å†³ç­–å·¥å…·è°ƒç”¨ |
| **çŠ¶æ€ç´¯åŠ ** | è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å² |

---

### ğŸš€ ä¸‹ä¸€æ­¥æ–¹å‘

#### åŠŸèƒ½å¢å¼º
- [ ] æ·»åŠ  Web ç•Œé¢ (Streamlit/Gradio)
- [ ] å®ç°æµå¼å“åº”
- [ ] æ”¯æŒå¤šæ¨¡æ€ (å›¾ç‰‡ã€è¯­éŸ³)
- [ ] é›†æˆæ›´å¤šå·¥å…· (æ•°æ®åº“ã€API)

#### æ¶æ„ä¼˜åŒ–
- [ ] å¤šä»£ç†åä½œç³»ç»Ÿ
- [ ] ä»»åŠ¡è§„åˆ’å’Œæ‰§è¡Œåˆ†ç¦»
- [ ] é•¿æœŸè®°å¿†å’Œå‘é‡æ•°æ®åº“

#### ç”Ÿäº§éƒ¨ç½²
- [ ] Docker å®¹å™¨åŒ–
- [ ] äº‘å¹³å°éƒ¨ç½² (Google Cloud Run, AWS Lambda)
- [ ] ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ
- [ ] è´Ÿè½½å‡è¡¡å’Œæ‰©å±•

---

**Happy Building! ğŸ‰**

---

*æœ¬æŒ‡å—ç”± AI ç”Ÿæˆ,æ¬¢è¿æå‡ºæ”¹è¿›å»ºè®®!*
